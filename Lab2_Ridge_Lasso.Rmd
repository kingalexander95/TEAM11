---
title: "Lab 2. Ridge Regression and Lasso"
author: "Group 11"
date: "1/27/2021"
output: 
  pdf_document:
    toc: no
  word_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(Matrix)
library(glmnet)
library(ISLR)
```

## 6.6 Loading Data and removing missing observations
We will perform ridge regression and the lasso in order to predict Salary on
the Hitters data. Before proceeding ensure that the missing values have
been removed from the data.

```{r }
sum(is.na(Hitters))
Hitters = na.omit(Hitters)
sum(is.na(Hitters))
```

The model.matrix() function is particularly useful for creating x; not only
does it produce a matrix corresponding to the 19 predictors but it also
automatically transforms any qualitative variables into dummy variables.
The latter property is important because glmnet() can only take numerical,
quantitative inputs.

```{r }
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
```


## 6.6.1 Ridge Regression
The glmnet() function has an alpha argument that determines what type
of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1
then a lasso model is fit. We first fit a ridge regression model.
```{r }
grid=10^seq(10,-2,length =100)
ridge.mod=glmnet (x,y,alpha=0, lambda=grid)
```
Associated with each value of lambda is a vector of ridge regression coefficients,
stored in a matrix that can be accessed by coef(). In this case, it is a 20x100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100
columns (one for each value of lambda).
```{r}
dim(coef(ridge.mod))
```
We expect the coefficient estimates to be much smaller, in terms of l2 norm,
when a large value of lambda is used, as compared to when a small value of lambda is
used. These are the coefficients when lambda = 11,498, along with their l2 norm:

```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[ ,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```

In contrast, here are the coefficients when  lambda= 705, along with their l2
norm. Note the much larger l2 norm of the coefficients associated with this
smaller value of lambda.

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[ ,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```
We can use the predict() function for a number of purposes. For instance,
we can obtain the ridge regression coefficients for a new value of lambda, say 50:

```{r}
predict(ridge.mod,s=50,type="coefficients")[1:20,]
```
We now split the samples into a training set and a test set in order
to estimate the test error of ridge regression and the lasso.

```{r}
set.seed(1)
train=sample(1: nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
```

We fit a ridge regression model on the training set, and evaluate
its MSE on the test set, using lambda = 4. We use the predict()
function again. This time we get predictions for a test set, by replacing
type="coefficients" with the newx argument.

```{r}
ridge.mod = glmnet(x[train, ],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod, s=4, newx=x[test, ])
mean((ridge.pred-y.test)^2)
```
Note that if we had instead simply fit a model with just an intercept, 
we would have predicted each test observation using the mean of the training observations. 
In that case, we could compute the test set MSE like this:
```{r}
mean((mean(y[train])-y.test)^2)
```
We could also get the same result by fitting a ridge regression model with
a very large value of lambda, like 10^10
```{r}
ridge.pred=predict(ridge.mod, s=1e10,newx = x[test, ])
mean((ridge.pred-y.test)^2)
```
So fitting a ridge regression model with lambda = 4 leads to a much lower test
MSE than fitting a model with just an intercept. We now check whether
there is any benefit to performing ridge regression with lambda = 4 instead of
just performing least squares regression. Least squares is simply
ridge regression with ?? = 0.
```{r}
ridge.pred=predict(ridge.mod,s=0,newx=x[test ,])
mean((ridge.pred-y.test)^2)
lm(y~x,subset=train)
predict(ridge.mod,s=0,type="coefficients")[1:20,]
```

In general, instead of arbitrarily choosing lambda = 4, it would be better to
use cross-validation to choose the tuning parameter lambda. We can do this using
the built-in cross-validation function, cv.glmnet().By default it performs ten-fold cross-validation. We can change it with argument nfolds.
```{r}
set.seed(1)
cv.out=cv.glmnet(x[train ,],y[ train],alpha=0)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
```

The test MSE associated with the best lambda value
```{r}
ridge.pred=predict(ridge.mod,s=bestlam,newx = x[test, ])
mean((ridge.pred-y.test)^2)
```
This represents a further improvement over the test MSE that we got using
lambda = 4. Finally, we refit our ridge regression model on the full data set,
using the value of lambda chosen by cross-validation, and examine the coefficient
estimates.
```{r}
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
```
As expected, none of the coefficients are zero-ridge regression does not
perform variable selection!

## 6.6.2 The Lasso
In order to fit a lasso model, we once again
use the glmnet() function; however, this time we use the argument alpha=1
```{r}
lasso.mod=glmnet(x[train ,],y[train],alpha=1, lambda =grid)
plot(lasso.mod)
```
We can see from the coefficient plot that depending on the choice of tuning
parameter, some of the coefficients will be exactly equal to zero. We now
perform cross-validation and compute the associated test error.
```{r}
set.seed(1)
cv.out=cv.glmnet(x[train, ],y[train],alpha=1)
plot(cv.out)
bestlam =cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```

This is substantially lower than the test set MSE of the null model and of
least squares, and very similar to the test MSE of ridge regression with lambda
chosen by cross-validation.
However, the lasso has a substantial advantage over ridge regression in
that the resulting coefficient estimates are sparse. Here we see that 12 of
the 19 coefficient estimates are exactly zero. So the lasso model with lambda
chosen by cross-validation contains only seven variables.
```{r}
out=glmnet (x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam) [1:20,]
lasso.coef
length(lasso.coef[lasso.coef!=0])
```